{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwTPfD-_mGwe"
      },
      "source": [
        "# Homework 2: Naive Bayes Email Spam Filter\n",
        "\n",
        "In this homework, we will learn how to implement the Naive Bayes classifier in order to create a simple email spam filter. This spam filter will be trained by test_emails, which will be given by a vector of tuples (emails, spam/nospam). For each tuple the first entry is a string (\"email\"), and the second entry is 0 or 1, depending whether the email contains spam words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcYY9uEZ8NNA"
      },
      "source": [
        "dictionary = ['hello', 'students',  'please', 'learn', 'for', 'the', 'exam', 'buy', 'drugs', 'today', 'sun', 'is', 'shining', 'in', 'nagoya', 'lets', 'sell', 'how', 'are', 'you', 'today?', 'do', 'your', 'homework', 'want', 'free', 'solutions?', 'hey', 'always', 'ask', 'questions', 'if','have', 'any', 'math', 'not', 'good', 'submit', 'pay'] \n",
        "\n",
        "test_emails=[\n",
        "             [\"hello students, please learn for the exam\", 0],\n",
        "             [\"hello students, please buy drugs\", 1],    \n",
        "             [\"hello, today the sun is shining in nagoya\", 0],\n",
        "             [\"lets sell drugs in nagoya\", 1],\n",
        "             [\"today learn drugs\", 1],\n",
        "             [\"how are you today?\", 0],\n",
        "             [\"hello students, please do your homework\", 0],\n",
        "             [\"hello, do you want free homework solutions?\", 1],\n",
        "             [\"hey students, please always ask questions if you have any\", 0],\n",
        "             [\"math is not good\", 1],\n",
        "             [\"math is good\", 0],\n",
        "             [\"please submit your homework\", 0],\n",
        "             [\"please buy questions\", 1],\n",
        "             [\"please pay for the exam\", 1]          \n",
        "             ]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-ba1vs7neJS",
        "outputId": "2a7481f1-ff39-43c2-d72b-994d287892d6"
      },
      "source": [
        "len(test_emails)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIbVy8Spndmq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7_jFoKsHlXZ"
      },
      "source": [
        "The feature space for our spam filter will be $\\mathcal{X}=\\{0,1\\}^d$, where $d$ denotes the number of words in the dictionary. For a feature (email) $x \\in \\mathcal{X}$ the entry $x_i$ for $i=1,\\dots,d$ is $1$ if the $i$-th word of the dictionary is contained in the email and $0$ otherwise.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihFEvEcCHI6y"
      },
      "source": [
        "# **Exercise 1**\n",
        "Implement a function which creates a feature vector out of an email and a function which creates a training set out of test emails. \n",
        "\n",
        "You would need to figure out whether a sentence contains a word, and there are functions in Python that could determine whether a string contains another string. You can consult documentation (and Google) to find out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcrE1LTy8mjy"
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "def email_to_feature(dict, email):\n",
        "  split_email=email[0]\n",
        "  feature=np.zeros(len(dict))\n",
        "  for i in range(len(dict)):\n",
        "    if dict[i] in split_email:\n",
        "      feature[i]=1\n",
        "  return(feature)\n",
        "def emails_to_test_samples(dict, test_emails):\n",
        "  training_set=copy.deepcopy(test_emails)\n",
        "  for i in range (len(test_emails)):\n",
        "    training_set[i][0]=email_to_feature(dictionary,test_emails[i])\n",
        "  return training_set\n",
        "  "
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIGHXPNkHxHW"
      },
      "source": [
        "below are for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dbkbu74pqsq",
        "outputId": "b9cc79d2-651f-421b-8ac3-d70c8e402805"
      },
      "source": [
        "emails_to_test_samples(dictionary,test_emails)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[array([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0.]), 0],\n",
              " [array([1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0.]), 1],\n",
              " [array([1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0.]), 0],\n",
              " [array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0.]), 1],\n",
              " [array([0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0.]), 1],\n",
              " [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0.]), 0],\n",
              " [array([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0.]), 0],\n",
              " [array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0.]), 1],\n",
              " [array([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         0., 0., 0., 0., 0.]), 0],\n",
              " [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 1., 1., 0., 0.]), 1],\n",
              " [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 0., 1., 0., 0.]), 0],\n",
              " [array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 1., 0.]), 0],\n",
              " [array([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0.]), 1],\n",
              " [array([0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1.]), 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzUtdPFZ_1vR"
      },
      "source": [
        " **Recall from Lecture 6:**\n",
        "\n",
        "Given a training set  $\\mathcal{T} = \\left( (x^{(1)}, y^{(1)}) , \\dots, (x^{(n)}, y^{(n)})   \\right)$ we calculate for $i=1,\\dots,d$ the following numbers\n",
        "\\begin{align*}\n",
        "\\phi_{i\\mid y=1} &= \\frac{1+\\sum_{j=1}^n I(x^{(j)}_i = 1  \\,\\wedge \\, y^{(j)}=1 ) }{2+\\sum_{j=1}^n I(y^{(j)}=1)}\\,,\\\\\n",
        "\t\\phi_{i\\mid y=0} &= \\frac{1+\\sum_{j=1}^n I(x^{(j)}_i = 1  \\,\\wedge \\, y^{(j)}=0 )}{2+\\sum_{j=1}^n I(y^{(j)}=0)}\\,,\\\\\n",
        "\t\t\\phi_{y=1} &= \\frac{1+\\sum_{j=1}^n I(y^{(j)} = 1)}{2+n} \\,.\n",
        "\\end{align*}\n",
        "Here $I$ denoted the indicator function. We used the laplace smoothing (thats why we have the $1+$ and $2+$) in order to make sure that we will not assume probability $0$ for unknown words.\n",
        "\n",
        "Now assume we get a new feature (i.e. someone sends us an email) $x \\in \\{0,1\\}^d$. Then we can calculate for each word $i=1,\\dots,d$ the probabilities\n",
        "\\begin{align*}\n",
        "P(x_i = 1 \\mid y=1) &= \\phi_{i\\mid y=1}\\,,\\qquad &&P(x_i = 1 \\mid y=0) = \\phi_{i\\mid y=0} \\,,\\\\\n",
        "P(x_i = 0 \\mid y=1) &= 1- \\phi_{i\\mid y=1}\\,,\\qquad &&P(x_i = 0 \\mid y=0) = 1-\\phi_{i\\mid y=0} \\,. \\\\\n",
        "\\end{align*}\n",
        "\n",
        "By the Naive Bayes assumption we have for $x \\in \\{0,1\\}^d$\n",
        "\\begin{align*}\n",
        "P(x \\mid y)  = \\prod_{i=1}^d P(x_j \\mid y)\\,.\n",
        "\\end{align*}\n",
        "The probability of the new email being spam is then\n",
        "\\begin{align*}\n",
        "P(y=1 \\mid x) &= \\frac{ P(x \\mid y=1) P(y=1)}{P(x)}\\\\\n",
        "&= \\frac{\\prod_{i=1}^d P(x_i \\mid y = 1) \\cdot \\phi_{y=1}}{\\prod_{i=1}^d P(x_i \\mid y = 1) \\cdot \\phi_{y=1} + \\prod_{i=1}^d P(x_i \\mid y = 0) (1-\\phi_{y=1})}\\,.\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KdBdWJQINbe"
      },
      "source": [
        "# **Exercise 2:** \n",
        "Use the above explanation of the Naive Bayes Spam filter and implement a function which gives the probability of an email being spam given the trainings email above. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ry9w9OtOFBi"
      },
      "source": [
        "# You can implement the above algorithm in any way you want.\n",
        "# One possible way:\n",
        "# - Calculate the phis here\n",
        "# - Write functions for the probability P(x|y) depending on these phis\n",
        "# - Use this function in the function spam_percentage \n",
        "def phi_i_y1(dict,training_set):\n",
        "  Sample=emails_to_test_samples(dict,training_set)\n",
        "  X=np.zeros(len(dict))\n",
        "  for i in range(len(X)):\n",
        "    num=1\n",
        "    denum=2\n",
        "    for j in range(len(Sample)):\n",
        "      if Sample[j][0][i]==1 and Sample[j][1]==1:\n",
        "        num+=1\n",
        "      if Sample[j][1]==1:\n",
        "        denum+=1\n",
        "    X[i]=num/denum\n",
        "  return X\n",
        "\n",
        "def phi_i_y0(dict,training_set):\n",
        "  Sample=emails_to_test_samples(dict,training_set)\n",
        "  X=np.zeros(len(dict))\n",
        "  for i in range(len(X)):\n",
        "    num=1\n",
        "    denum=2\n",
        "    for j in range(len(Sample)):\n",
        "      if Sample[j][0][i]==1 and Sample[j][1]==0:\n",
        "        num+=1\n",
        "      if Sample[j][1]==0:\n",
        "        denum+=1\n",
        "    X[i]=num/denum\n",
        "  return X\n",
        "\n",
        "def phi_y1(training_set):\n",
        "  num=1\n",
        "  denum=2+len(training_set)\n",
        "  for j in range (len(training_set)):\n",
        "    if training_set[j][1]==1:\n",
        "      num+=1\n",
        "  return num/denum\n",
        "\n",
        "def spam_percentage(email):\n",
        "  A=phi_i_y0(dictionary,test_emails)\n",
        "  B=phi_i_y1(dictionary,test_emails)\n",
        "  C=phi_y1(test_emails)\n",
        "  num=C\n",
        "  denum2=1-C\n",
        "  for i in range(len(A)):\n",
        "    if dictionary[i] in email:\n",
        "      num=num*B[i]\n",
        "      denum2=denum2*A[i]\n",
        "    else:\n",
        "      num=num*(1-B[i])\n",
        "      denum2=denum2*(1-A[i])\n",
        "  prob=num/(num+denum2)\n",
        "\n",
        "  return prob  \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZbjSUit1Q9J",
        "outputId": "f3d4577d-bcc3-4d49-ccb1-c5c0054ebe05"
      },
      "source": [
        "print(phi_i_y1(dictionary,test_emails))\r\n",
        "print(phi_i_y0(dictionary,test_emails))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.33333333 0.22222222 0.44444444 0.22222222 0.22222222 0.22222222\n",
            " 0.22222222 0.33333333 0.44444444 0.22222222 0.11111111 0.22222222\n",
            " 0.11111111 0.22222222 0.22222222 0.22222222 0.22222222 0.11111111\n",
            " 0.11111111 0.22222222 0.11111111 0.22222222 0.11111111 0.22222222\n",
            " 0.22222222 0.22222222 0.22222222 0.11111111 0.11111111 0.11111111\n",
            " 0.22222222 0.11111111 0.11111111 0.11111111 0.22222222 0.22222222\n",
            " 0.22222222 0.11111111 0.22222222]\n",
            "[0.44444444 0.44444444 0.55555556 0.22222222 0.22222222 0.33333333\n",
            " 0.22222222 0.11111111 0.11111111 0.33333333 0.22222222 0.33333333\n",
            " 0.22222222 0.22222222 0.22222222 0.11111111 0.11111111 0.22222222\n",
            " 0.22222222 0.55555556 0.22222222 0.22222222 0.33333333 0.33333333\n",
            " 0.11111111 0.11111111 0.11111111 0.22222222 0.22222222 0.22222222\n",
            " 0.22222222 0.22222222 0.22222222 0.22222222 0.22222222 0.11111111\n",
            " 0.22222222 0.22222222 0.11111111]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-0XJ4jt1QwE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2ABCIxrT11Z"
      },
      "source": [
        "Test your spam filter with the following email.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo_egC7AUYoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb118a82-8ccc-4455-d9a4-e72abb441063"
      },
      "source": [
        "email=\"the sun is shining. buy drugs now\"\n",
        "print(spam_percentage(email))\n",
        "print(spam_percentage(\"buy drugs now\"))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9299429164504411\n",
            "0.9953136390460652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82y_1mNLP6Lu"
      },
      "source": [
        "\n",
        "# **Exercise 3**\n",
        "Extend your spamfilter by creating a dynamical dictionary. Instead of starting with a fixed dictionary, you should now create a dictionary out of a list of emails. \n",
        "\n",
        "Write a function `create_dictionary(emails)` which returns a dictionary created from a list of emails (Give as an array of arrays `[text, spam\\nospam]`). Make sure that you do not include words more than once into the dictionary.\n",
        "To implement this function you should look up the function `split()` for a string in Python. To take care of the symbols \".\" and \",\" you can use the `replace()` function of a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rvAqAkHz21D"
      },
      "source": [
        "import re\n",
        "def create_dictionary(emails):\n",
        "  dict=[]\n",
        "  r='[,.?;/]'\n",
        "  for i in range(len(emails)):\n",
        "    dict0=re.split(r,emails[i][0])\n",
        "    for j in range(len(dict0)):\n",
        "      dict1=dict0[j].split()\n",
        "      for k in range(len(dict1)):\n",
        "        if dict1[k] not in dict:\n",
        "          dict.append(dict1[k])\n",
        "  print(dict)\n",
        "  return dict"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtWAvWjmMPPN",
        "outputId": "5708d179-aa2f-41c6-f4cc-9bf00ea9e688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_emails = [\r\n",
        "                 ['chickens are good for your heart', 1],\r\n",
        "                 ['chickens are being killed everyday for your meat, stop eating meat', 0],\r\n",
        "                 ['i am writing english', 0],\r\n",
        "                 ['i am not writing english', 0],\r\n",
        "                 ['i need to go to the toilet', 1],\r\n",
        "                 ['i hate your liver', 1],\r\n",
        "                 ['your exam is on monday', 1],\r\n",
        "                 ['let me see your liver and sell it', 1],\r\n",
        "                 ['this is urgent so please message me back', 0],\r\n",
        "                 ['please donate five hundred dollars for my shoes', 1],\r\n",
        "                 ['i am nigerian prince', 1],\r\n",
        "                 ['i am nigerian', 0],\r\n",
        "                 ['the meeting is on tuesday', 0],\r\n",
        "                 [\"hello students, please learn for the exam\", 0],\r\n",
        "                 [\"hello students, please buy drugs\", 1],    \r\n",
        "                 [\"hello, today the sun is shining in nagoya\", 0],\r\n",
        "                 [\"lets sell drugs in nagoya\", 1],\r\n",
        "                 [\"today learn drugs\", 1],\r\n",
        "                 [\"how are you today?\", 0],\r\n",
        "                 [\"hello students, please do your homework\", 0],\r\n",
        "                 [\"hello, do you want free homework solutions?\", 1],\r\n",
        "                 [\"hey students, please always ask questions if you have any\", 0],\r\n",
        "                 [\"math is not good\", 1],\r\n",
        "                 [\"math is good\", 0],\r\n",
        "                 [\"please submit your homework\", 0],\r\n",
        "                 [\"please buy questions\", 1],\r\n",
        "                 [\"please pay for the exam\", 1],    \r\n",
        "]\r\n",
        "\r\n",
        "create_dictionary(sample_emails)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['chickens', 'are', 'good', 'for', 'your', 'heart', 'being', 'killed', 'everyday', 'meat', 'stop', 'eating', 'i', 'am', 'writing', 'english', 'not', 'need', 'to', 'go', 'the', 'toilet', 'hate', 'liver', 'exam', 'is', 'on', 'monday', 'let', 'me', 'see', 'and', 'sell', 'it', 'this', 'urgent', 'so', 'please', 'message', 'back', 'donate', 'five', 'hundred', 'dollars', 'my', 'shoes', 'nigerian', 'prince', 'meeting', 'tuesday', 'hello', 'students', 'learn', 'buy', 'drugs', 'today', 'sun', 'shining', 'in', 'nagoya', 'lets', 'how', 'you', 'do', 'homework', 'want', 'free', 'solutions', 'hey', 'always', 'ask', 'questions', 'if', 'have', 'any', 'math', 'submit', 'pay']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chickens',\n",
              " 'are',\n",
              " 'good',\n",
              " 'for',\n",
              " 'your',\n",
              " 'heart',\n",
              " 'being',\n",
              " 'killed',\n",
              " 'everyday',\n",
              " 'meat',\n",
              " 'stop',\n",
              " 'eating',\n",
              " 'i',\n",
              " 'am',\n",
              " 'writing',\n",
              " 'english',\n",
              " 'not',\n",
              " 'need',\n",
              " 'to',\n",
              " 'go',\n",
              " 'the',\n",
              " 'toilet',\n",
              " 'hate',\n",
              " 'liver',\n",
              " 'exam',\n",
              " 'is',\n",
              " 'on',\n",
              " 'monday',\n",
              " 'let',\n",
              " 'me',\n",
              " 'see',\n",
              " 'and',\n",
              " 'sell',\n",
              " 'it',\n",
              " 'this',\n",
              " 'urgent',\n",
              " 'so',\n",
              " 'please',\n",
              " 'message',\n",
              " 'back',\n",
              " 'donate',\n",
              " 'five',\n",
              " 'hundred',\n",
              " 'dollars',\n",
              " 'my',\n",
              " 'shoes',\n",
              " 'nigerian',\n",
              " 'prince',\n",
              " 'meeting',\n",
              " 'tuesday',\n",
              " 'hello',\n",
              " 'students',\n",
              " 'learn',\n",
              " 'buy',\n",
              " 'drugs',\n",
              " 'today',\n",
              " 'sun',\n",
              " 'shining',\n",
              " 'in',\n",
              " 'nagoya',\n",
              " 'lets',\n",
              " 'how',\n",
              " 'you',\n",
              " 'do',\n",
              " 'homework',\n",
              " 'want',\n",
              " 'free',\n",
              " 'solutions',\n",
              " 'hey',\n",
              " 'always',\n",
              " 'ask',\n",
              " 'questions',\n",
              " 'if',\n",
              " 'have',\n",
              " 'any',\n",
              " 'math',\n",
              " 'submit',\n",
              " 'pay']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    }
  ]
}